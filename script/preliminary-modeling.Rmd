---
title: "Preliminary Modeling"
author: "Matthew Wiens"
output: pdf_document
date: '2023-05-24'
editor_options: 
  chunk_output_type: console
---

# Setup



Load libraries and set up file paths.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(xgboost)

data_dir <- here::here("data", "source")
```

Data source: https://archive.ics.uci.edu/ml/datasets/breast+cancer

```{R}

# Pull names from the data description file, since the csv does not have column names
input_column_names <- c(
  "Class",
  "age",
  "menopause",
  "tumor_size",
  "inv_nodes",
  "node_caps",
  "deg_malig",
  "breast",
  "breast_quad",
  "irradiat"
)

# Load data, with options for this specific dataset
dat_raw <- read_csv(
  file = file.path(data_dir, "breast-cancer.data"),
  col_names = input_column_names,
  na = c("", "NA", "?")
) 
  

```

Data description:

   1. Class: no-recurrence-events, recurrence-events
   2. age: 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, 90-99.
   3. menopause: lt40, ge40, premeno.
   4. tumor-size: 0-4, 5-9, 10-14, 15-19, 20-24, 25-29, 30-34, 35-39, 40-44,
                  45-49, 50-54, 55-59.
   5. inv-nodes: 0-2, 3-5, 6-8, 9-11, 12-14, 15-17, 18-20, 21-23, 24-26,
                 27-29, 30-32, 33-35, 36-39.
   6. node-caps: yes, no.
   7. deg-malig: 1, 2, 3.
   8. breast: left, right.
   9. breast-quad: left-up, left-low, right-up, right-low, central.
  10. irradiat: yes, no.
  
  
Cleanup the ordered categories to be numeric here  
```{R}

# Further preprocessing, using tidyverse
# Trees naturally handle ordered factors (categorical) covariates,
# so we code the data in that way
# The importance_weights column will be used later in the script
dat <- dat_raw %>%
  # TODO:(andrew, matthew) drop any rows where a variable is NA?
  #     : Sounds like we want to keep this in. But we should probably
  #     : be ready to discuss this with participants, i.e., what is
  #     : XGBoost doing with missing data.
  # drop_na() %>%
  mutate(
    # define logical variable for whether subject had 
    # recurrence events (TRUE) or not (FALSE)
    class_n = Class == "recurrence-events",
    # define Class as a factor with levels that match class_n, 
    # so recurrence_events is factor level 1
    Class = factor(Class, 
                   levels = c("recurrence-events", "no-recurrence-events")),
    # define age as an ordered factor
    age_f = factor(age, ordered = TRUE),
    # define tumor size as an ordered factor with specified levels
    tumor_size_f = factor(tumor_size,
                          ordered = TRUE, 
                          levels = c("0-4", "5-9", "10-14", "15-19",
                                     "20-24", "25-29", "30-34", "35-39", 
                                     "40-44", "45-49",  "50-54")),
    # define nodes as an ordered factor with specified levels
    inv_nodes_f = factor(inv_nodes, 
                         ordered = TRUE,
                         levels = c("0-2", "3-5", "6-8", "9-11", 
                                    "12-14", "15-17", "24-26")),
    # define logical variable for whether subject receieved radiation (TRUE)
    # or not (FALSE)
    irradiat_n = irradiat == "yes",
    # define logical variable for whether node caps were used (TRUE) 
    # or not (FALSE)
    node_caps_n = node_caps == "yes",
    # define logical variable for whether tumor was in left brease (TRUE)
    # or not (FALSE)
    breast_left = breast == "left",
    # define importance weights
    # TODO(matthew): Can we comment on how these are defined/chosen?
    case_weight = importance_weights(if_else(class_n == 0, 1, 201/85))
  )

# Create the training/testing split with the tidymodels package
# will use default proportion for 75% (3/4) training, 25% testing
init_split <- initial_split(dat, prop = 3/4)
```


Create the formatted datasets for training and testing in a format for **xgboost**, which is a matrix. 
Also, the **recipes** package is used to preprocess categorical and ordinal covariates into numerics for **xgboost**.

```{R}

# This is like specifying a model, but for data pre-proecessing 
model_formula <- as.formula(
  class_n ~ age_f + menopause + tumor_size_f + inv_nodes_f + node_caps_n + 
            deg_malig + breast_left + breast_quad + irradiat_n
)
recipe_xgboost <- recipes::recipe(
  model_formula, 
  # call the training portion of the split data set using training()
  # extraction function
  data = training(init_split)) %>%
  # convert the ordinal factors into numeric scores (e.g., 0, 1, 2)
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  # create dummy variables non-reference levels of factors, the one_hot = TRUE
  # option tells the function to assume a reference level and then make
  # numerics for the other levels. For example, if you have N levels, then
  # N-1 dummy variables are created.
  step_dummy(breast_quad, menopause, one_hot = TRUE)

# This is like fitting the model 
# TODO(matthew): Why does the data have to be specified here and in the
#   recipe? Same question for below.
estimated_preprocessing <- prep(recipe_xgboost, training(init_split))

# This is like using the model to make predictions
# TODO(matthew): I'm confused by the comment above. Isn't this just pulling
#   out the design matrix and responses for training and test?
x_train <- bake(estimated_preprocessing, training(init_split)) %>% 
  select(-class_n) %>% 
  as.matrix()
y_train <- bake(estimated_preprocessing, training(init_split)) %>%
  pull(class_n) 

x_test <- bake(estimated_preprocessing, testing(init_split)) %>% 
  select(-class_n) %>% 
  as.matrix()
y_test <- bake(estimated_preprocessing, testing(init_split)) %>% 
  pull(class_n) 

```

Fit the model with default tuning parameters. 

```{R}
# Fit the model, specifying that we are doing binary classification and thus
# want to "score" models to minimized their logistic loss. Note that
# this is not specifying logistic regression, rather specifying that the loss
# function to be minimized is logistic.
fit1 <- xgboost::xgboost(data = x_train,  # predictors
                         label = y_train, # responses
                         # nrounds: number of boosted trees to add to ensemble
                         #          can be tuned, but fixed here for demo
                         nrounds = 15,  
                         # see note above about logistic loss function
                         params = list(objective = "binary:logistic"))

```

Evaluate fit

```{R}

# Using the test set here, which was not used to fit the model, meaning
# this is out-of-sample validation/prediction
predictions <- testing(init_split) %>%
  mutate(pred_pr = predict(fit1, x_test))

# AUC (area under the ROC curve) is a common way to assess a binary prediction
# 0.5 = model no better than a random classifier; the closer AUC gets to 
# 1 (a perfect classifier), the better the model
predictions %>%
  mutate(truth = Class) %>%
  roc_auc(pred_pr, truth = truth, estimator = "binary", event_level = "first")

# Confusion matrix for predictions
predictions %>%
  mutate(truth = Class,
         pred = factor(if_else(pred_pr >= 0.5,
                               "no-recurrence-events", 
                               "recurrence-events", ), 
                       levels = c("recurrence-events", 
                                  "no-recurrence-events"))) %>%
  yardstick::conf_mat(estimate = pred, truth = truth)

# The yardstick package has many functions to evaluate (and plot evaluation) 
# of models, and works in the tidymodels package

```


# Hyperparameter Tuning and Tidymodels


Source: https://juliasilge.com/blog/xgboost-tune-volleyball/
```{R}

# TODO(matthew): What is getting changed here besides imputation?
# Slightly tweak the formatting of the dataset for tidymodels format
# Also, for simplification of code, we'll just impute missing values
# Even though in the previous example xgboost cleanly handles them 
recipe_xgboost_tidymodels <- recipes::recipe(
  Class ~ age_f + menopause + tumor_size_f + inv_nodes_f + node_caps_n + 
    deg_malig + breast_left + breast_quad + irradiat_n, 
                data = training(init_split)) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  step_mutate(node_caps_n = as.numeric(node_caps_n), 
              breast_left = as.numeric(breast_left), 
              irradiat_n = as.numeric(irradiat_n)) %>%
  step_dummy(breast_quad, menopause, one_hot = T) 
  
# Using the recipes again
estimated_preprocessing2 <- prep(recipe_xgboost_tidymodels, training(init_split))

bake(estimated_preprocessing2, training(init_split))

# Everything that could be tuned, within reason
# But this isn't the most practical approach (curse of dimensionality)
# Also, note the tuning parameter names are slightly different in tidymodels
# compared to xgboost
xgb_spec_all <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = tune(),        
  sample_size = tune(), 
  mtry = tune(),        
  learn_rate = tune()      
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")


# Here's a more limited set of parameters to tune
xgb_spec <- boost_tree(
  trees = 15,
  tree_depth = tune(),
  min_n = tune(),
  loss_reduction = 0,        
  sample_size = 1.0, 
  learn_rate = 0.3      
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

# One way to select the points to tune
# See the help/google for more details
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  size = 50
)

# Combine preprocessing and fitting into a "workflow"
xgb_wf <- workflow() %>%
  add_recipe(recipe_xgboost_tidymodels) %>%
  add_model(xgb_spec)

# Set up cross-validation  
cv_folds <- vfold_cv(training(init_split), v = 5)

set.seed(1234)

# Question: How many models do we have to fit?

# Run the cross-validation
tuning_results <- tune_grid(
  xgb_wf,
  resamples = cv_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE, verbose = TRUE,  event_level = "first")
)

# Summary of the results
collect_metrics(tuning_results)

# Analysis of the results
# ROC AUC
best_auc <- select_best(tuning_results, "roc_auc")

# Create the final model, with actual values for the hyperparameters 
final_wf <- finalize_workflow(
  xgb_wf,
  best_auc
)

# Some ways to examine the final fit
final_fit <- final_wf %>%
  last_fit(init_split) 

final_fit %>%
  collect_predictions() %>% 
  roc_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

final_fit %>%
    collect_predictions() %>% 
    roc_auc(Class, `.pred_recurrence-events`, event_level = "first")

final_fit %>%
    collect_predictions() %>% 
    accuracy(Class, .pred_class)

# Question: Is this accuracy good for this dataset? What accuracy would a very naive algorithm have? 

# Answer: Well a model always predicting no events would have approx 70% accuracy
# SUGGESTION: Have participants generate confusion matrix corresponding the
# naive model of no events. Not modeling per se, but I think is important
# for building intuition around the topic.

# Question: What about focusing on the patients with recurrence-events? What are alternatives to ROC in the yardstick package for calculating an AUC?

# TODO: need to provide intuition for the PR curve? not as common as ROC
final_fit %>%
  collect_predictions() %>% 
  # set event_level = "first" because our event of interest is the 
  # `no-recurrence-event` factor level of Class, which is the first
  # level in Class.
  pr_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

final_fit %>%
  collect_predictions() %>% 
  # set event_level = "first" because our event of interest is the 
  # `no-recurrence-event` factor level of Class, which is the first
  # level in Class.
  pr_auc(Class, `.pred_recurrence-events`, event_level = "first")


```

## Also, something about imbalanced classes

```{R}

set.seed(12345)

recipe_xgboost_tidymodels_weights <- recipes::recipe(
  Class ~age_f + menopause + tumor_size_f + inv_nodes_f + node_caps_n + 
    deg_malig + breast_left + breast_quad + irradiat_n + case_weight, 
                data = training(init_split)) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_ordinalscore(age_f, tumor_size_f, inv_nodes_f) %>%
  step_mutate(node_caps_n = as.numeric(node_caps_n), 
              breast_left = as.numeric(breast_left), 
              irradiat_n = as.numeric(irradiat_n)) %>%
  step_dummy(breast_quad, menopause, one_hot = T) 

xgb_wf_weights <- xgb_wf %>%
  update_recipe(recipe_xgboost_tidymodels_weights) %>%
  add_case_weights(case_weight)

tuning_results_weights <- tune_grid(
  xgb_wf_weights,
  resamples = cv_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE, verbose = TRUE,  event_level = "first")
)

collect_metrics(tuning_results_weights)

# FIXME: getting error about checking value of `metric` b/c pr_auc
#        not in the metrics.
best_auc_weighted <- select_best(tuning_results_weights, "pr_auc")



# Create the final model, with actual values for the hyperparameters 
final_wf_weighted <- finalize_workflow(
  xgb_wf_weights,
  best_auc_weighted
)

final_fit_weighted <- final_wf_weighted %>%
  last_fit(init_split) 

final_fit_weighted %>%
  collect_predictions() %>% 
  pr_curve(Class, `.pred_recurrence-events`, event_level = "first") %>% 
  autoplot()

final_fit_weighted %>%
    collect_predictions() %>% 
    pr_auc(Class, `.pred_recurrence-events`, event_level = "first")


final_fit_weighted %>%
    collect_predictions() %>% 
    roc_auc(Class, `.pred_recurrence-events`, event_level = "first")
```


## Model interpretation with Shapley Values


```{R}

# Note the shapley values are for predicting the first class in the Class factor, which is no recurrence events
# Always good to pay attention to how R and libraries are using factor orderings
shap_plot <- xgb.ggplot.shap.summary(data = bake(extract_recipe(final_fit), testing(init_split)) %>% select(-Class) %>% as.matrix(),
                        model = extract_fit_parsnip(final_fit)$fit) +
  labs(x = "Shapley Value", color = "Feature Value") 

shap_plot



```

Sanity check the shapley predictions by making predictions for two different values of tumor sizes for one set of covariate values

```{R}

# Create a dataset of covariates to make predictions
x_predict <- testing(init_split)[1,] %>%
  select(-tumor_size_f) %>%
  dplyr::inner_join(tibble(tumor_size_f = c("0-4", "50-54")), by = character())


# Make the predictions
predict(extract_workflow(final_fit),
        new_data = x_predict,
        type = "prob")
```